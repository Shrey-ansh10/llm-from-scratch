{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f854c0b1",
   "metadata": {},
   "source": [
    "🔹 OpenWebText\n",
    "An open-source effort to replicate the dataset used to train GPT-2. It includes web pages linked from high-karma Reddit posts, excluding spam and low-quality content.\n",
    "\n",
    "Size: ~40 GB\n",
    "\n",
    "License: MIT\n",
    "\n",
    "Notes: Emphasizes quality text from reputable online sources.\n",
    "\n",
    "Link: https://www.kaggle.com/datasets/himonsarkar/openwebtext-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690703e3",
   "metadata": {},
   "source": [
    "Smaller part of the Big OpenwebText dataset.\n",
    "Dataset(stas/openwebtext-10k): https://huggingface.co/datasets/stas/openwebtext-10k \n",
    "- Openweb text is a pensource \n",
    "- This is a small dataset of 15MB compressed and 50MB uncompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a268eb7-4a8c-4219-b121-a6a6a03b516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #openweb \n",
    "# from datasets import load_dataset\n",
    "\n",
    "# ds = load_dataset(\"stas/openwebtext-10k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287aef6",
   "metadata": {},
   "source": [
    "### Step: 1 Loading text, splitting into tokens and cleaning\n",
    "\n",
    "(For now, using the-verdict.txt a book for learning basics, in second half will work on custom data i.e. \"openwebtext\" and apply learnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0001418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: #reading the file \n",
    "    raw_text = f.read() #storing in the variable raw_text\n",
    "\n",
    "print(\"Total number of characters: \", len(raw_text)) #print total number of characters\n",
    "print(raw_text[:99])  #printing first 100 characters, remember character not word and it includes spaces as well. If it works, means we have loaded data and we're ready for next step, splitting text into words and subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9231c0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### regular expression library (i.e. regex) is used for splitting text based on specific keyword or pattern.\n",
    "- A Regular Expression is a pattern-matching tool used to search, extract, replace, or manipulate strings/text data using specific character patterns.\n",
    "- regex: This module provides functions and tools to use regular expressions in Python.\n",
    "\n",
    "| Function                               | Description                            |\n",
    "| -------------------------------------- | -------------------------------------- |\n",
    "| `re.search(pattern, string)`           | Finds **first match**                  |\n",
    "| `re.findall(pattern, string)`          | Returns **all matches**                |\n",
    "| `re.sub(pattern, replacement, string)` | **Replaces** all matches               |\n",
    "| `re.split(pattern, string)`            | Splits string by pattern               |\n",
    "| `re.match(pattern, string)`            | Matches pattern **from the beginning** |\n",
    "\n",
    "-> Above are many functions, but we'll use only split to split our text data based on our need. \n",
    "\n",
    "#### Common Use Cases of Regex:\n",
    "| Task                   | Example Regex  | What it Does                                |\n",
    "| ---------------------- | -------------- | ------------------------------------------- |\n",
    "| **Find numbers**       | `\\d+`          | Matches one or more digits                  |\n",
    "| **Find words**         | `\\w+`          | Matches one or more word characters         |\n",
    "| **Whitespace**         | `\\s+`          | Matches spaces, tabs, newlines              |\n",
    "| **Email extraction**   | `\\w+@\\w+\\.\\w+` | Matches a simple email pattern              |\n",
    "| **Remove punctuation** | `[^\\w\\s]`      | Matches anything that’s NOT a word or space |\n",
    "\n",
    "\n",
    "##### 🧠 Why Is Regex Useful?\n",
    "- Data cleaning (e.g., remove HTML tags, punctuation)\n",
    "- Information extraction (e.g., extract dates, names)\n",
    "- Preprocessing for NLP (e.g., splitting sentences)\n",
    "- Validating input (e.g., form inputs like passwords or emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ade0b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# The following is a regex pattern designed to split a string on punctuation, whitespace, and dashes, while keeping the delimiters.\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "print(result[:99]) #this will still have spaces as individual tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a78084e",
   "metadata": {},
   "source": [
    "What's happening in the above code?\n",
    "1. r'...': The r prefix makes it a raw string — so backslashes (\\) are not interpreted as escape characters by Python.\n",
    "\n",
    "2. Outer parentheses (...): paranthesis after re.split(r'\n",
    "- This is a capturing group, which means re.split() will keep the delimiters in the result (instead of discarding them).\n",
    "\n",
    "3. Inside the group:\n",
    "\n",
    "| Part           | Meaning                                                                                                                                                           |\n",
    "| -------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `[,.:;?_!\"()]` | Matches **any single character** that is a comma, dot, colon, semicolon, question mark, underscore, exclamation mark, double quote, **open or close parenthesis** |\n",
    "| `--`           | Matches **double dash** (not a character class — it's a separate alternative)                                                                                     |\n",
    "| `\\s`           | Matches **any whitespace** (space, tab, newline, etc.)                                                                                                            |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42194452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me']\n"
     ]
    }
   ],
   "source": [
    "#to remove the spaces\n",
    "result = [item.strip() for item in result if item.strip()]  #We are calling item.strip(), which is a built-in string function in Python used to remove leading and trailing whitespace from a string. This is applied to each item in the result list. After stripping the whitespace, if the resulting string is not empty, it is added to the new list. If the stripped string is empty (meaning the item was either empty or contained only whitespace), it is skipped. The result is a new list containing only the cleaned, non-empty strings from the original result list.\n",
    "\n",
    "print(result[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8a07af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe802a",
   "metadata": {},
   "source": [
    "### Step 2: Creating Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7994c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1130\n"
     ]
    }
   ],
   "source": [
    "unique_words = sorted(set(result)) #set() will create a set of unique values from result, sorted() will produce a new sorted list which we'll store in words var\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a5e7c",
   "metadata": {},
   "source": [
    "#### -> Over here our vocab size is very small due to our small dataset, but LLMs are trained on very large data, therefore they have millions or even billions of tokens and their respective IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating vocabulary\n",
    "vocab = {token: integer for integer, token in enumerate(unique_words)}\n",
    "\n",
    "# enumerate(all_words) returns (index, token) pairs for each token in the result list.\n",
    "# for integer, token in enumerate(all_words) loops through that.\n",
    "# token: integer is the key-value pair for the dictionary.\n",
    "# { ... } wraps it into a dictionary comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a109b12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!: 0\n",
      "\": 1\n",
      "': 2\n",
      "(: 3\n",
      "): 4\n",
      ",: 5\n",
      "--: 6\n",
      ".: 7\n",
      ":: 8\n",
      ";: 9\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(vocab.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a396b",
   "metadata": {},
   "source": [
    "#### => So the main task starts now, we'll build a simple Tokenizer class that will help us automate the whole process from raw text to tokens to extracting token ID's from Vocab dictionary and vice versa. \n",
    "(remember this will give us a solid understanding of tokenizers and help later in building and creating advance tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a04b6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab): \n",
    "        self.str_to_int = vocab #vocab will be used to extract ID for input text tokens\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} #extracting words associated with tokenID produced by LLM \n",
    "\n",
    "    def encode(self, text): \n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) #creating tokens\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] # removing spaces\n",
    "        \n",
    "        ids = [self.str_to_int[s] for s in preprocessed] #as we already have a dictionary of words, we'll extract id from vocab for the input we get. \n",
    "        return ids\n",
    "        \n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19657b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 514, 149, 208, 549, 431, 1016, 530, 1112, 856, 949, 997, 722, 546, 8, 988, 420, 879, 198, 871, 362, 568, 412, 7]\n"
     ]
    }
   ],
   "source": [
    "#let's test our tokenizer \n",
    "testText = \"It had always been his fate to have women say such things of him: the fact should be set down in extenuation.\" \n",
    "# this text is from our training data, any token outside ouf the unique tokens in our vocab will produce error, and hence we need very big data.\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab) #creating a instane/object of our class\n",
    "\n",
    "ids = tokenizer.encode(testText)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a532a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It had always been his fate to have women say such things of him: the fact should be set down in extenuation.\n"
     ]
    }
   ],
   "source": [
    "#let's convert this IDs back to text tocheck out decoder\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0861797b",
   "metadata": {},
   "source": [
    "#### Special Context Tokens\n",
    "- To handel the tokens that doesn't exist in our dataset/vocab we will add a special token, such as <|unk|> <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cad1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';']\n"
     ]
    }
   ],
   "source": [
    "newtokens = sorted(set(result)) \n",
    "print(newtokens[:10])\n",
    "\n",
    "newtokens.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
    "\n",
    "vocab = {token: integer for integer, token in enumerate(newtokens)} #adding new tokens to vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac864af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yourself', '<|unk|>', '<|endoftext|>']\n"
     ]
    }
   ],
   "source": [
    "print(newtokens[-3:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8a4353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "729c1b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yourself: 1129\n",
      "<|unk|>: 1130\n",
      "<|endoftext|>: 1131\n"
     ]
    }
   ],
   "source": [
    "for i, (key, value) in enumerate(list(vocab.items())[-3:]):\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b1d3d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab): \n",
    "        self.str_to_int = vocab #vocab will be used to extract ID for input text tokens\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()} #extracting words associated with tokenID produced by LLM \n",
    "\n",
    "    def encode(self, text): \n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text) #creating tokens\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()] # removing spaces\n",
    "        \n",
    "        #to handel tokens which are not in our dict, and avoid crashing our program\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int #item represent a token in our list preprocessed, if that token exist in vocab it stays as it is\n",
    "            else \"<|unk|>\" for item in preprocessed # else it is converted to <|unk|>\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed] #as we already have a dictionary of words, we'll extract id from vocab for the input we get. \n",
    "        return ids\n",
    "        \n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2d5c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1130, 5, 1089, 169, 1126, 357, 10, 53, 469, 988, 296, 180, 975, 215, 989, 751]\n"
     ]
    }
   ],
   "source": [
    "#testing the new implementation\n",
    "tokenizer2 = SimpleTokenizerV2(vocab)\n",
    "testText2 = \"Hi, what are you doing? I found the couple at tea beneath their palm-trees\"\n",
    "\n",
    "print(tokenizer2.encode(testText2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d52cbaac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, what are you doing? I found the couple at tea beneath their palm-trees'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(tokenizer2.encode(testText2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65886a84",
   "metadata": {},
   "source": [
    "-> When an out of vocab word is encountered, then encoding it as unk won't really help and affect model output.\n",
    "\n",
    "-> So to deal with this, \"byte pair encoding\" is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4d552",
   "metadata": {},
   "source": [
    "### Short summary of all the steps performed till now.\n",
    "1. Split text into individual tokens, using regular expression.\n",
    "2. Removed whitespace.\n",
    "3. then created a sorted set of unique tokens.\n",
    "4. Now based on this sorted set of unique tokens, created a vocab with each word having a unique token ID.\n",
    "5. Created a tokenizer class, that used this vocab, of tokens and their IDs to encode text to tokenIDs and decode tokenID to text. \n",
    "\n",
    " ____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86296ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66e2dfde",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8bcde3c",
   "metadata": {},
   "source": [
    "# Working with custom dataset. i.e. Openwebtext-10k\n",
    "- Importing a smaller version of openweb text data.\n",
    "- This is a very large dataset compared to \"the-verdict\" book pdf.\n",
    "- Therefore, it will require better regular expression to seperate various types of token in a better way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4814c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "666e79d3af8342138c9227a4dbb4a923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "443017f6b3944299b7463f6d6ecd0e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openwebtext-10k.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66c069302a742e18f229e7dc2531b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/30.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b250a90535f2457ca0c1082ea3741339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#openweb \n",
    "from datasets import load_dataset\n",
    "\n",
    "#loading dataset\n",
    "ds = load_dataset(\"stas/openwebtext-10k\", cache_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8f9351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#exploring data\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "49ef3077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['text']}\n"
     ]
    }
   ],
   "source": [
    "print(ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53624554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"A magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\\n\\nThe city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\\n\\nWhat it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\\n\\nBut in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\\n\\nNo law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\\n\\nBut the question of whether to publish it in the country where Hitler plotted his empire has lost some of its edge in the Google era, when a complete German-language copy of the book pops up as the second result on the local version of the search engine.\\n\\n“To say this is a very dangerous book, we must ban it, this is ridiculous,” said Wolfgang Wippermann, a professor of modern history at the Free University of Berlin. “Maybe it was necessary once, but now it’s over, it makes no sense. You can find it so easily.”\\n\\nThe publisher of the excerpts, London-based Albertus, has said it will appeal the Bavarian government’s injunction. In 2009, the publisher beat charges of copyright violation and the illegal use of Nazi symbols after the Bavarian government seized reprinted copies of the Nazi Party’s in-house newspaper.\\n\\nThe attempt to publish portions of “Mein Kampf” on Thursday was scuttled at the last moment, although the publisher, ready to capitalize on the publicity, had printed two versions of the pamphlet. The version propped on top of a heap of celebrity magazines at a newsstand in Berlin’s central Friedrichstrasse station was a slender, blue, 16-page leaflet that has historical commentary in one column and an image of blurred text stamped with “Unreadable” in the other, accompanied by two reproductions of Nazi-era newspapers.\\n\\n“Mein Kampf” “is an awful book, and the whole thinking is absolutely not ours, but we have another view on it regarding the idea of packing it away. This idea is just naive,” said Alexander Luckow, a spokesman for the publisher. “In a free country, you need to discuss these very bad parts of German history.”\\n\\nStill, he said, there are limits, and using Hitler’s words as inspiration, not as historical artifact, is where it crosses the line.\\n\\n“The danger is allowing right-wing people to sell it in bookshops with their modern commentary,” he said. “This is forbidden and it’s good . . . not only in Germany, this should be equal in other countries in Europe. Anti-Semitism is not confined to Germany. You look and it’s all around Europe, dating back to the Middle Ages.”\\n\\nThe debate will soon be over, whether or not the latest excerpts make it to newsstands. German law extends copyright 70 years after the author’s death; after 2015, “Mein Kampf” will be fair game. Some in Bavaria’s government worry that neo-Nazis will publish their own version of the book shortly thereafter, and to counter that, they are encouraging a scholarly edition. A group of historians is preparing it.\\n\\nGermany’s Jewish organizations have approached the publication with mixed emotions, sensitive that their country still has problems with neo-Nazis and anti-Semitism. The German government released a study this week that found that one in five Germans has anti-Semitic attitudes. And a neo-Nazi ring that has been linked to at least nine killings before it was shut down in November shocked Germans who thought they had done a thorough job working through their past.\\n\\n“I do very well without any publishing of ‘Mein Kampf,’ ” said Dieter Graumann, the head of the Central Council of Jews in Germany. “In a few years, it will be free, and I have every trust in the democratic maturity of the German people. . . . But for the moment, I am glad it is not.”\"}\n"
     ]
    }
   ],
   "source": [
    "#dataset has one feature and 10K entries/rows\n",
    "#printing to see what it looks like\n",
    "print(ds['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52290707",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ds[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "89c6419b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text']\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e3e82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_ds)) #length of data             \n",
    "train_ds.features  #to see the format/structure of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95181fb9",
   "metadata": {},
   "source": [
    "#### \"train_ds\" (i.e. ds[\"train\"] part of while dataset), is a Dataset object with:\n",
    "- 10,000 dictionaries, where each dictionary is one data sample (row), with 'text' as key and the 'sentence' as value\n",
    "[\n",
    " \n",
    "  {'text': 'sentence 1'},\n",
    " \n",
    "  {'text': 'sentence 2'},\n",
    " \n",
    "  ...\n",
    " \n",
    "  {'text': 'sentence 10000'}\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a339a396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\n",
      "\n",
      "The city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\n",
      "\n",
      "What it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\n",
      "\n",
      "But in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\n",
      "\n",
      "No law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\n",
      "\n",
      "But the question of whether to publish it in the country where Hitler plotted his empire has lost some of its edge in the Google era, when a complete German-language copy of the book pops up as the second result on the local version of the search engine.\n",
      "\n",
      "“To say this is a very dangerous book, we must ban it, this is ridiculous,” said Wolfgang Wippermann, a professor of modern history at the Free University of Berlin. “Maybe it was necessary once, but now it’s over, it makes no sense. You can find it so easily.”\n",
      "\n",
      "The publisher of the excerpts, London-based Albertus, has said it will appeal the Bavarian government’s injunction. In 2009, the publisher beat charges of copyright violation and the illegal use of Nazi symbols after the Bavarian government seized reprinted copies of the Nazi Party’s in-house newspaper.\n",
      "\n",
      "The attempt to publish portions of “Mein Kampf” on Thursday was scuttled at the last moment, although the publisher, ready to capitalize on the publicity, had printed two versions of the pamphlet. The version propped on top of a heap of celebrity magazines at a newsstand in Berlin’s central Friedrichstrasse station was a slender, blue, 16-page leaflet that has historical commentary in one column and an image of blurred text stamped with “Unreadable” in the other, accompanied by two reproductions of Nazi-era newspapers.\n",
      "\n",
      "“Mein Kampf” “is an awful book, and the whole thinking is absolutely not ours, but we have another view on it regarding the idea of packing it away. This idea is just naive,” said Alexander Luckow, a spokesman for the publisher. “In a free country, you need to discuss these very bad parts of German history.”\n",
      "\n",
      "Still, he said, there are limits, and using Hitler’s words as inspiration, not as historical artifact, is where it crosses the line.\n",
      "\n",
      "“The danger is allowing right-wing people to sell it in bookshops with their modern commentary,” he said. “This is forbidden and it’s good . . . not only in Germany, this should be equal in other countries in Europe. Anti-Semitism is not confined to Germany. You look and it’s all around Europe, dating back to the Middle Ages.”\n",
      "\n",
      "The debate will soon be over, whether or not the latest excerpts make it to newsstands. German law extends copyright 70 years after the author’s death; after 2015, “Mein Kampf” will be fair game. Some in Bavaria’s government worry that neo-Nazis will publish their own version of the book shortly thereafter, and to counter that, they are encouraging a scholarly edition. A group of historians is preparing it.\n",
      "\n",
      "Germany’s Jewish organizations have approached the publication with mixed emotions, sensitive that their country still has problems with neo-Nazis and anti-Semitism. The German government released a study this week that found that one in five Germans has anti-Semitic attitudes. And a neo-Nazi ring that has been linked to at least nine killings before it was shut down in November shocked Germans who thought they had done a thorough job working through their past.\n",
      "\n",
      "“I do very well without any publishing of ‘Mein Kampf,’ ” said Dieter Graumann, the head of the Central Council of Jews in Germany. “In a few years, it will be free, and I have every trust in the democratic maturity of the German people. . . . But for the moment, I am glad it is not.”\n"
     ]
    }
   ],
   "source": [
    "print(train_ds['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f06fd88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: A magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\n",
      "\n",
      "The city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\n",
      "\n",
      "What it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\n",
      "\n",
      "But in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\n",
      "\n",
      "No law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\n",
      "\n",
      "But the question of whether to publish it in the country where Hitler plotted his empire has lost some of its edge in the Google era, when a complete German-language copy of the book pops up as the second result on the local version of the search engine.\n",
      "\n",
      "“To say this is a very dangerous book, we must ban it, this is ridiculous,” said Wolfgang Wippermann, a professor of modern history at the Free University of Berlin. “Maybe it was necessary once, but now it’s over, it makes no sense. You can find it so easily.”\n",
      "\n",
      "The publisher of the excerpts, London-based Albertus, has said it will appeal the Bavarian government’s injunction. In 2009, the publisher beat charges of copyright violation and the illegal use of Nazi symbols after the Bavarian government seized reprinted copies of the Nazi Party’s in-house newspaper.\n",
      "\n",
      "The attempt to publish portions of “Mein Kampf” on Thursday was scuttled at the last moment, although the publisher, ready to capitalize on the publicity, had printed two versions of the pamphlet. The version propped on top of a heap of celebrity magazines at a newsstand in Berlin’s central Friedrichstrasse station was a slender, blue, 16-page leaflet that has historical commentary in one column and an image of blurred text stamped with “Unreadable” in the other, accompanied by two reproductions of Nazi-era newspapers.\n",
      "\n",
      "“Mein Kampf” “is an awful book, and the whole thinking is absolutely not ours, but we have another view on it regarding the idea of packing it away. This idea is just naive,” said Alexander Luckow, a spokesman for the publisher. “In a free country, you need to discuss these very bad parts of German history.”\n",
      "\n",
      "Still, he said, there are limits, and using Hitler’s words as inspiration, not as historical artifact, is where it crosses the line.\n",
      "\n",
      "“The danger is allowing right-wing people to sell it in bookshops with their modern commentary,” he said. “This is forbidden and it’s good . . . not only in Germany, this should be equal in other countries in Europe. Anti-Semitism is not confined to Germany. You look and it’s all around Europe, dating back to the Middle Ages.”\n",
      "\n",
      "The debate will soon be over, whether or not the latest excerpts make it to newsstands. German law extends copyright 70 years after the author’s death; after 2015, “Mein Kampf” will be fair game. Some in Bavaria’s government worry that neo-Nazis will publish their own version of the book shortly thereafter, and to counter that, they are encouraging a scholarly edition. A group of historians is preparing it.\n",
      "\n",
      "Germany’s Jewish organizations have approached the publication with mixed emotions, sensitive that their country still has problems with neo-Nazis and anti-Semitism. The German government released a study this week that found that one in five Germans has anti-Semitic attitudes. And a neo-Nazi ring that has been linked to at least nine killings before it was shut down in November shocked Germans who thought they had done a thorough job working through their past.\n",
      "\n",
      "“I do very well without any publishing of ‘Mein Kampf,’ ” said Dieter Graumann, the head of the Central Council of Jews in Germany. “In a few years, it will be free, and I have every trust in the democratic maturity of the German people. . . . But for the moment, I am glad it is not.”\n",
      "----------------------------------------\n",
      "Row 1: For today’s post, I’d like to take a look at California’s voter initiative to legalize pot. If the measure passes, and the sky doesn’t fall, many other states will probably be looking at similar law changes in the near future. Our drug policy of the last century has simply not worked, and it’s heartening to see a state attempting to legalize marijuana.\n",
      "\n",
      "The statistics on marijuana arrests are really shocking. According to the Drug Policy Alliance, which is in favor of legalization, blacks are arrested for marijuana possession between four and twelve times more than whites in California, even though studies have consistently shown that whites smoke more pot than blacks. In the last ten years, around 500,000 people have been arrested for possession. That’s absurd! Think about how expensive that is for the criminal justice system. California spends $216,000 for each juvenile inmate in its prison system, yet it spends only $8,000 per student in the Oakland school system. It seems to me that if you really want to limit drug use, it’d make more sense to spend more money keeping kids in school, helping them achieve.\n",
      "\n",
      "The economic benefits of legalizing marijuana are mind blowing. If marijuana was legalized and taxed at the same rate of tobacco, the money we would save on law enforcement and gain in tax revenue equals about $17 billion. As Nicholas Kristof notes, that is enough money to send every three and four year old in a poor neighborhood to pre-school. Or we could spend that money improving public school education. Or we could use the money to shore up border defense. Whatever we do, $17 billion is not exactly a trivial amount.\n",
      "\n",
      "For me, the biggest reason to legalize marijuana is to hurt the cartels. Immigration has emerged as a hot button issue recently, with Arizona passing a draconian immigration law and many similar propositions being considered by other states. People are worried about violence, and understandably so. No one wants to have foreign drug dealers operating in their back yard. But no matter how many laws we pass, or how much money we spend, marijuana from Mexico and other Latin American countries will always find a way across the border. Drug importers are smart, and the demand is so high that increased patrols by border agents and harsher prison sentences will not act as an effective deterrent. America will always have a demand for marijuana, and that means as long as the drug stays illegal, violent drug cartels will operate in our borders.\n",
      "\n",
      "But what if the drug that the cartels are pushing is suddenly legal? No one in their right mind would buy pot off the street if they could instead walk into a dispensary and buy high quality marijuana legally, and probably for less money than the cartels are charging. Very few people actually want to have to hide their drug use. If given a choice, marijuana smokers would absolutely buy legal drugs. This would severely weaken the cartels, and decrease deaths related to drug trafficking.\n",
      "\n",
      "I’m not advocating drug use here. I know people who have ruined their lives from excess drug use. But it’s not true that marijuana is the gateway drug that people have been demonizing for years. Just because someone smokes pot every once in a while doesn’t mean that person will turn around and become a heroin addict. Yes, marijuana intoxicates you, but so do legal drugs like alcohol. As long as sensible restrictions are built into the law, such as making it illegal to drive under the influence, then there is no reason that marijuana should not be legalized.\n",
      "----------------------------------------\n",
      "Row 2: Anarchists in solidarity with the purged immigrants of Agios Panteleimonas ventured once again to open the public playground which is kept locked by fascists in favor of segregation, leading to battle with riot police and five arrests.\n",
      "\n",
      "On Tuesday 9/06 anarchists in solidarity to immigrants who are being daily terrorised by fascist thugs of the Golden Dawn neonazi party and their local allies in the area of Agios Panteleimonas, moved to unblock the entrance of the local children playground which the fascists want to keep locked in an effort to impose segregation between greeks and immigrants, and \"to preserve the blood purity of the white race\"...While unblocking the playground the anarchists were attacked by fascists who were soon routed before the arrival of riot police forces who engaged the anarchists in battle with the aim of protecting the fascists. During the clashes one policeman was injured and five protesters were arrested on criminal charges. After the end of the clashes, a local greek father, Mr Tasoulas, defying the reign of terror in the area, took his son to play in the coveted playground. Soon they were surrounded by fascists who blocked the exit of the playground and threatened to linch the father calling him a traitor. After he managed to handle the child to a sympathetic neighbor, the fascists beat the father in full presence of the chief of the local police station. The strong police forces present at the scene then arrested the father and took him to the local police station, where his solicitor, a leading figure of the legal world and human rights activist, was piled with eggs by fascists who threatened her life.\n",
      "\n",
      "The new tension in the area comes after the euroelection ascent of LAOS, the fascist Popular Orthodox Alarm Party, to the 4th position with 7% of the vote. This in combination with the governing party's landslide defeat, has led the government to endorse the core of the extreme-right wing policies of LAOS, and pledge a mass sweeping operation against illegal immigrants and their greek supporters within the summer. As the concentration camp planned to be built in the old NATO airbase of Aspropyrgos is now deemed impractical, the government has committed several old military camps of disgraceful humanitarian standards around the capital for the purpose of \"cleaning the city of foreigners\". The measures and discourse comes as little surprise as it comes from a political party famous for wanting to displace homosexuals in desert islands in the late 1970s. Furthermore the \"Law and Order\" operation of the coming summer is said to also include a mass attack against anarchist squats, and solidarity actions to immigrants by the movement as a whole. This the government hopes to achieve via a virtual military occupation of the center of Athens for the summer months modeled on Olympics 2004, as well as the introduction of a disputed legislation that would eventually render protest marches illegal. Due to the lack of a legislative majority by one MP, the government has resorted to yet another legal trick by increasing the total number of MPs by one, non-elected member of its own liking for the summer session of the Parliament.\n",
      "\n",
      "The dictatorial rule of the right-wing and the ruthless employment of its parastate agents is increasing the tension across the country. Last week one police station in Athens was attacked and a central tax office was bombed by a Marxist guerrilla group, while a series of luxury brothels frequented by the ruling class were destroyed. At the same time, the movement is on its guard in expectation of next Saturday's Gay Pride parade which last year was attacked by parastate fascist thugs, as well as in expectation of an evacuation of the old courts in down-town Athens which are occupied by immigrants and are a constant target by the bourgeois media who waste no time in supporting the fascists in a most unambiguous manner.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Row {i}:\", train_ds[i][\"text\"])\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a0974",
   "metadata": {},
   "source": [
    "#### Now, applying tokenization in a way I've learned till now\n",
    "- Splitting texts to token.\n",
    "- Creating a new vocab for token ID's and creating a tokeinzer class based on this new bigger vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382558be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd101c108ff4d7cae0809e99605c3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#building a function to split text to tokens and remove whitespaces\n",
    "#as we have multiple rows of data, we'll call this function for each row and convert texts in those rows to tokens\n",
    "\n",
    "def text_to_token(text):\n",
    "    tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text[\"text\"]) #using the regular expresion to split text to token\n",
    "    tokens = [t.strip() for t in tokens if t.strip()] #removing whitespaces\n",
    "    return {\"tokens\": tokens} #returning a key value pair, with key=\"tokens\" and tokenized text as value\n",
    "\n",
    "tokenized_ds = train_ds.map(text_to_token) #.map function will apply our text_to_token function to each item in dataset and returns a map object, which is an iterator containing the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "feca9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'tokens'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "\n",
      "Following is the raw text from 1st row :\n",
      "For today’s post, I’d like to take a look at California’s voter initiative to legalize pot. If the measure passes, and the sky doesn’t fall, many other states will probably be looking at similar law changes in the near future. Our drug policy of the last century has simply not worked, and it’s heartening to see a state attempting to legalize marijuana.\n",
      "\n",
      "The statistics on marijuana arrests are really shocking. According to the Drug Policy Alliance, which is in favor of legalization, blacks are arrested for marijuana possession between four and twelve times more than whites in California, even though studies have consistently shown that whites smoke more pot than blacks. In the last ten years, around 500,000 people have been arrested for possession. That’s absurd! Think about how expensive that is for the criminal justice system. California spends $216,000 for each juvenile inmate in its prison system, yet it spends only $8,000 per student in the Oakland school system. It seems to me that if you really want to limit drug use, it’d make more sense to spend more money keeping kids in school, helping them achieve.\n",
      "\n",
      "The economic benefits of legalizing marijuana are mind blowing. If marijuana was legalized and taxed at the same rate of tobacco, the money we would save on law enforcement and gain in tax revenue equals about $17 billion. As Nicholas Kristof notes, that is enough money to send every three and four year old in a poor neighborhood to pre-school. Or we could spend that money improving public school education. Or we could use the money to shore up border defense. Whatever we do, $17 billion is not exactly a trivial amount.\n",
      "\n",
      "For me, the biggest reason to legalize marijuana is to hurt the cartels. Immigration has emerged as a hot button issue recently, with Arizona passing a draconian immigration law and many similar propositions being considered by other states. People are worried about violence, and understandably so. No one wants to have foreign drug dealers operating in their back yard. But no matter how many laws we pass, or how much money we spend, marijuana from Mexico and other Latin American countries will always find a way across the border. Drug importers are smart, and the demand is so high that increased patrols by border agents and harsher prison sentences will not act as an effective deterrent. America will always have a demand for marijuana, and that means as long as the drug stays illegal, violent drug cartels will operate in our borders.\n",
      "\n",
      "But what if the drug that the cartels are pushing is suddenly legal? No one in their right mind would buy pot off the street if they could instead walk into a dispensary and buy high quality marijuana legally, and probably for less money than the cartels are charging. Very few people actually want to have to hide their drug use. If given a choice, marijuana smokers would absolutely buy legal drugs. This would severely weaken the cartels, and decrease deaths related to drug trafficking.\n",
      "\n",
      "I’m not advocating drug use here. I know people who have ruined their lives from excess drug use. But it’s not true that marijuana is the gateway drug that people have been demonizing for years. Just because someone smokes pot every once in a while doesn’t mean that person will turn around and become a heroin addict. Yes, marijuana intoxicates you, but so do legal drugs like alcohol. As long as sensible restrictions are built into the law, such as making it illegal to drive under the influence, then there is no reason that marijuana should not be legalized.\n",
      "\n",
      "Following are the tokens of the raw text of 1st row :\n",
      "['For', 'today’s', 'post', ',', 'I’d', 'like', 'to', 'take', 'a', 'look', 'at', 'California’s', 'voter', 'initiative', 'to', 'legalize', 'pot', '.', 'If', 'the', 'measure', 'passes', ',', 'and', 'the', 'sky', 'doesn’t', 'fall', ',', 'many', 'other', 'states', 'will', 'probably', 'be', 'looking', 'at', 'similar', 'law', 'changes', 'in', 'the', 'near', 'future', '.', 'Our', 'drug', 'policy', 'of', 'the', 'last', 'century', 'has', 'simply', 'not', 'worked', ',', 'and', 'it’s', 'heartening', 'to', 'see', 'a', 'state', 'attempting', 'to', 'legalize', 'marijuana', '.', 'The', 'statistics', 'on', 'marijuana', 'arrests', 'are', 'really', 'shocking', '.', 'According', 'to', 'the', 'Drug', 'Policy', 'Alliance', ',', 'which', 'is', 'in', 'favor', 'of', 'legalization', ',', 'blacks', 'are', 'arrested', 'for', 'marijuana', 'possession', 'between', 'four', 'and', 'twelve', 'times', 'more', 'than', 'whites', 'in', 'California', ',', 'even', 'though', 'studies', 'have', 'consistently', 'shown', 'that', 'whites', 'smoke', 'more', 'pot', 'than', 'blacks', '.', 'In', 'the', 'last', 'ten', 'years', ',', 'around', '500', ',', '000', 'people', 'have', 'been', 'arrested', 'for', 'possession', '.', 'That’s', 'absurd', '!', 'Think', 'about', 'how', 'expensive', 'that', 'is', 'for', 'the', 'criminal', 'justice', 'system', '.', 'California', 'spends', '$216', ',', '000', 'for', 'each', 'juvenile', 'inmate', 'in', 'its', 'prison', 'system', ',', 'yet', 'it', 'spends', 'only', '$8', ',', '000', 'per', 'student', 'in', 'the', 'Oakland', 'school', 'system', '.', 'It', 'seems', 'to', 'me', 'that', 'if', 'you', 'really', 'want', 'to', 'limit', 'drug', 'use', ',', 'it’d', 'make', 'more', 'sense', 'to', 'spend', 'more', 'money', 'keeping', 'kids', 'in', 'school', ',', 'helping', 'them', 'achieve', '.', 'The', 'economic', 'benefits', 'of', 'legalizing', 'marijuana', 'are', 'mind', 'blowing', '.', 'If', 'marijuana', 'was', 'legalized', 'and', 'taxed', 'at', 'the', 'same', 'rate', 'of', 'tobacco', ',', 'the', 'money', 'we', 'would', 'save', 'on', 'law', 'enforcement', 'and', 'gain', 'in', 'tax', 'revenue', 'equals', 'about', '$17', 'billion', '.', 'As', 'Nicholas', 'Kristof', 'notes', ',', 'that', 'is', 'enough', 'money', 'to', 'send', 'every', 'three', 'and', 'four', 'year', 'old', 'in', 'a', 'poor', 'neighborhood', 'to', 'pre-school', '.', 'Or', 'we', 'could', 'spend', 'that', 'money', 'improving', 'public', 'school', 'education', '.', 'Or', 'we', 'could', 'use', 'the', 'money', 'to', 'shore', 'up', 'border', 'defense', '.', 'Whatever', 'we', 'do', ',', '$17', 'billion', 'is', 'not', 'exactly', 'a', 'trivial', 'amount', '.', 'For', 'me', ',', 'the', 'biggest', 'reason', 'to', 'legalize', 'marijuana', 'is', 'to', 'hurt', 'the', 'cartels', '.', 'Immigration', 'has', 'emerged', 'as', 'a', 'hot', 'button', 'issue', 'recently', ',', 'with', 'Arizona', 'passing', 'a', 'draconian', 'immigration', 'law', 'and', 'many', 'similar', 'propositions', 'being', 'considered', 'by', 'other', 'states', '.', 'People', 'are', 'worried', 'about', 'violence', ',', 'and', 'understandably', 'so', '.', 'No', 'one', 'wants', 'to', 'have', 'foreign', 'drug', 'dealers', 'operating', 'in', 'their', 'back', 'yard', '.', 'But', 'no', 'matter', 'how', 'many', 'laws', 'we', 'pass', ',', 'or', 'how', 'much', 'money', 'we', 'spend', ',', 'marijuana', 'from', 'Mexico', 'and', 'other', 'Latin', 'American', 'countries', 'will', 'always', 'find', 'a', 'way', 'across', 'the', 'border', '.', 'Drug', 'importers', 'are', 'smart', ',', 'and', 'the', 'demand', 'is', 'so', 'high', 'that', 'increased', 'patrols', 'by', 'border', 'agents', 'and', 'harsher', 'prison', 'sentences', 'will', 'not', 'act', 'as', 'an', 'effective', 'deterrent', '.', 'America', 'will', 'always', 'have', 'a', 'demand', 'for', 'marijuana', ',', 'and', 'that', 'means', 'as', 'long', 'as', 'the', 'drug', 'stays', 'illegal', ',', 'violent', 'drug', 'cartels', 'will', 'operate', 'in', 'our', 'borders', '.', 'But', 'what', 'if', 'the', 'drug', 'that', 'the', 'cartels', 'are', 'pushing', 'is', 'suddenly', 'legal', '?', 'No', 'one', 'in', 'their', 'right', 'mind', 'would', 'buy', 'pot', 'off', 'the', 'street', 'if', 'they', 'could', 'instead', 'walk', 'into', 'a', 'dispensary', 'and', 'buy', 'high', 'quality', 'marijuana', 'legally', ',', 'and', 'probably', 'for', 'less', 'money', 'than', 'the', 'cartels', 'are', 'charging', '.', 'Very', 'few', 'people', 'actually', 'want', 'to', 'have', 'to', 'hide', 'their', 'drug', 'use', '.', 'If', 'given', 'a', 'choice', ',', 'marijuana', 'smokers', 'would', 'absolutely', 'buy', 'legal', 'drugs', '.', 'This', 'would', 'severely', 'weaken', 'the', 'cartels', ',', 'and', 'decrease', 'deaths', 'related', 'to', 'drug', 'trafficking', '.', 'I’m', 'not', 'advocating', 'drug', 'use', 'here', '.', 'I', 'know', 'people', 'who', 'have', 'ruined', 'their', 'lives', 'from', 'excess', 'drug', 'use', '.', 'But', 'it’s', 'not', 'true', 'that', 'marijuana', 'is', 'the', 'gateway', 'drug', 'that', 'people', 'have', 'been', 'demonizing', 'for', 'years', '.', 'Just', 'because', 'someone', 'smokes', 'pot', 'every', 'once', 'in', 'a', 'while', 'doesn’t', 'mean', 'that', 'person', 'will', 'turn', 'around', 'and', 'become', 'a', 'heroin', 'addict', '.', 'Yes', ',', 'marijuana', 'intoxicates', 'you', ',', 'but', 'so', 'do', 'legal', 'drugs', 'like', 'alcohol', '.', 'As', 'long', 'as', 'sensible', 'restrictions', 'are', 'built', 'into', 'the', 'law', ',', 'such', 'as', 'making', 'it', 'illegal', 'to', 'drive', 'under', 'the', 'influence', ',', 'then', 'there', 'is', 'no', 'reason', 'that', 'marijuana', 'should', 'not', 'be', 'legalized', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds)\n",
    "print(\"\\nFollowing is the raw text from 1st row :\")\n",
    "print(tokenized_ds['text'][1]) #prints 1st row of text column\n",
    "print(\"\\nFollowing are the tokens of the raw text of 1st row :\")\n",
    "print(tokenized_ds['tokens'][1]) #prints 1st row of tokens column, here there are multiple tokens in form of list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7a6628",
   "metadata": {},
   "source": [
    "- So what has happened is, we have tokenized text and stored it as key:value pair in the new dataset var \"tokenized_ds\".\n",
    "- for each text, we have respective tokens as new feature/column in dataset.\n",
    "\n",
    "-> But there is a problem I am observing in the above tokens, token \"today's\" is not split into three seperate tokens, and when I researched about why, I found The ’ in \"today’s\" is a curly apostrophe (Unicode: U+2019), not a straight ' (ASCII: 39).\n",
    "\n",
    "=> To fix this there are 2 options:\n",
    "    \n",
    "    -> Update the regular expression to handle Unicode apostrophes like ’, ‘, and possibly other Unicode punctuation as following: \n",
    "        re.split(r'([,.:;?_!\"()\\']|--|\\s|’|‘)', text[\"text\"])\n",
    "    \n",
    "    -> go broader using Unicode categories (requires regex module instead of re):\n",
    "        import regex  # install with `pip install regex`\n",
    "        regex.split(r'(\\p{P}|\\s)', text)  # splits on any Unicode punctuation or whitespace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9596c540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac168af83c54376ab6a253a66ca4cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'tokens'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#update-2\n",
    "import regex  # install with `pip install regex`\n",
    "\n",
    "def text_to_token(text):\n",
    "    tokens = regex.split(r'(\\p{P}|\\s)', text[\"text\"])  # splits on any Unicode punctuation or whitespace\n",
    "    tokens = [t.strip() for t in tokens if t.strip()] #removing whitespaces\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "tokenized_ds = train_ds.map(text_to_token)\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1e4c2bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['For', 'today', '’', 's', 'post', ',', 'I', '’', 'd', 'like', 'to', 'take', 'a', 'look', 'at', 'California', '’', 's', 'voter', 'initiative', 'to', 'legalize', 'pot', '.', 'If', 'the', 'measure', 'passes', ',', 'and', 'the', 'sky', 'doesn', '’', 't', 'fall', ',', 'many', 'other', 'states', 'will', 'probably', 'be', 'looking', 'at', 'similar', 'law', 'changes', 'in', 'the', 'near', 'future', '.', 'Our', 'drug', 'policy', 'of', 'the', 'last', 'century', 'has', 'simply', 'not', 'worked', ',', 'and', 'it', '’', 's', 'heartening', 'to', 'see', 'a', 'state', 'attempting', 'to', 'legalize', 'marijuana', '.', 'The', 'statistics', 'on', 'marijuana', 'arrests', 'are', 'really', 'shocking', '.', 'According', 'to', 'the', 'Drug', 'Policy', 'Alliance', ',', 'which', 'is', 'in', 'favor', 'of', 'legalization', ',', 'blacks', 'are', 'arrested', 'for', 'marijuana', 'possession', 'between', 'four', 'and', 'twelve', 'times', 'more', 'than', 'whites', 'in', 'California', ',', 'even', 'though', 'studies', 'have', 'consistently', 'shown', 'that', 'whites', 'smoke', 'more', 'pot', 'than', 'blacks', '.', 'In', 'the', 'last', 'ten', 'years', ',', 'around', '500', ',', '000', 'people', 'have', 'been', 'arrested', 'for', 'possession', '.', 'That', '’', 's', 'absurd', '!', 'Think', 'about', 'how', 'expensive', 'that', 'is', 'for', 'the', 'criminal', 'justice', 'system', '.', 'California', 'spends', '$216', ',', '000', 'for', 'each', 'juvenile', 'inmate', 'in', 'its', 'prison', 'system', ',', 'yet', 'it', 'spends', 'only', '$8', ',', '000', 'per', 'student', 'in', 'the', 'Oakland', 'school', 'system', '.', 'It', 'seems', 'to', 'me', 'that', 'if', 'you', 'really', 'want', 'to', 'limit', 'drug', 'use', ',', 'it', '’', 'd', 'make', 'more', 'sense', 'to', 'spend', 'more', 'money', 'keeping', 'kids', 'in', 'school', ',', 'helping', 'them', 'achieve', '.', 'The', 'economic', 'benefits', 'of', 'legalizing', 'marijuana', 'are', 'mind', 'blowing', '.', 'If', 'marijuana', 'was', 'legalized', 'and', 'taxed', 'at', 'the', 'same', 'rate', 'of', 'tobacco', ',', 'the', 'money', 'we', 'would', 'save', 'on', 'law', 'enforcement', 'and', 'gain', 'in', 'tax', 'revenue', 'equals', 'about', '$17', 'billion', '.', 'As', 'Nicholas', 'Kristof', 'notes', ',', 'that', 'is', 'enough', 'money', 'to', 'send', 'every', 'three', 'and', 'four', 'year', 'old', 'in', 'a', 'poor', 'neighborhood', 'to', 'pre', '-', 'school', '.', 'Or', 'we', 'could', 'spend', 'that', 'money', 'improving', 'public', 'school', 'education', '.', 'Or', 'we', 'could', 'use', 'the', 'money', 'to', 'shore', 'up', 'border', 'defense', '.', 'Whatever', 'we', 'do', ',', '$17', 'billion', 'is', 'not', 'exactly', 'a', 'trivial', 'amount', '.', 'For', 'me', ',', 'the', 'biggest', 'reason', 'to', 'legalize', 'marijuana', 'is', 'to', 'hurt', 'the', 'cartels', '.', 'Immigration', 'has', 'emerged', 'as', 'a', 'hot', 'button', 'issue', 'recently', ',', 'with', 'Arizona', 'passing', 'a', 'draconian', 'immigration', 'law', 'and', 'many', 'similar', 'propositions', 'being', 'considered', 'by', 'other', 'states', '.', 'People', 'are', 'worried', 'about', 'violence', ',', 'and', 'understandably', 'so', '.', 'No', 'one', 'wants', 'to', 'have', 'foreign', 'drug', 'dealers', 'operating', 'in', 'their', 'back', 'yard', '.', 'But', 'no', 'matter', 'how', 'many', 'laws', 'we', 'pass', ',', 'or', 'how', 'much', 'money', 'we', 'spend', ',', 'marijuana', 'from', 'Mexico', 'and', 'other', 'Latin', 'American', 'countries', 'will', 'always', 'find', 'a', 'way', 'across', 'the', 'border', '.', 'Drug', 'importers', 'are', 'smart', ',', 'and', 'the', 'demand', 'is', 'so', 'high', 'that', 'increased', 'patrols', 'by', 'border', 'agents', 'and', 'harsher', 'prison', 'sentences', 'will', 'not', 'act', 'as', 'an', 'effective', 'deterrent', '.', 'America', 'will', 'always', 'have', 'a', 'demand', 'for', 'marijuana', ',', 'and', 'that', 'means', 'as', 'long', 'as', 'the', 'drug', 'stays', 'illegal', ',', 'violent', 'drug', 'cartels', 'will', 'operate', 'in', 'our', 'borders', '.', 'But', 'what', 'if', 'the', 'drug', 'that', 'the', 'cartels', 'are', 'pushing', 'is', 'suddenly', 'legal', '?', 'No', 'one', 'in', 'their', 'right', 'mind', 'would', 'buy', 'pot', 'off', 'the', 'street', 'if', 'they', 'could', 'instead', 'walk', 'into', 'a', 'dispensary', 'and', 'buy', 'high', 'quality', 'marijuana', 'legally', ',', 'and', 'probably', 'for', 'less', 'money', 'than', 'the', 'cartels', 'are', 'charging', '.', 'Very', 'few', 'people', 'actually', 'want', 'to', 'have', 'to', 'hide', 'their', 'drug', 'use', '.', 'If', 'given', 'a', 'choice', ',', 'marijuana', 'smokers', 'would', 'absolutely', 'buy', 'legal', 'drugs', '.', 'This', 'would', 'severely', 'weaken', 'the', 'cartels', ',', 'and', 'decrease', 'deaths', 'related', 'to', 'drug', 'trafficking', '.', 'I', '’', 'm', 'not', 'advocating', 'drug', 'use', 'here', '.', 'I', 'know', 'people', 'who', 'have', 'ruined', 'their', 'lives', 'from', 'excess', 'drug', 'use', '.', 'But', 'it', '’', 's', 'not', 'true', 'that', 'marijuana', 'is', 'the', 'gateway', 'drug', 'that', 'people', 'have', 'been', 'demonizing', 'for', 'years', '.', 'Just', 'because', 'someone', 'smokes', 'pot', 'every', 'once', 'in', 'a', 'while', 'doesn', '’', 't', 'mean', 'that', 'person', 'will', 'turn', 'around', 'and', 'become', 'a', 'heroin', 'addict', '.', 'Yes', ',', 'marijuana', 'intoxicates', 'you', ',', 'but', 'so', 'do', 'legal', 'drugs', 'like', 'alcohol', '.', 'As', 'long', 'as', 'sensible', 'restrictions', 'are', 'built', 'into', 'the', 'law', ',', 'such', 'as', 'making', 'it', 'illegal', 'to', 'drive', 'under', 'the', 'influence', ',', 'then', 'there', 'is', 'no', 'reason', 'that', 'marijuana', 'should', 'not', 'be', 'legalized', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds['tokens'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f48db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '$$', '$$$$', '$$3222111000', '$0', '$0$', '$0`', '$0x0', '$0x1', '$0x1020', '$0x20', '$0x5', '$0x6', '$1', '$1$', '$10', '$100', '$100+', '$1000', '$100K', '$100M', '$100NL']\n"
     ]
    }
   ],
   "source": [
    "#collecting all tokens in a single list\n",
    "all_tokens = [token for row in tokenized_ds \n",
    "                        for token in row[\"tokens\"]]\n",
    "\n",
    "#creating a sorted unique tokens list\n",
    "unique_tokens = sorted(set(all_tokens))\n",
    "\n",
    "print(unique_tokens[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a3c3de91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184468\n",
      "['!', '\"', '#', '$', '$$', '$$$$', '$$3222111000', '$0', '$0$', '$0`', '$0x0', '$0x1', '$0x1020', '$0x20', '$0x5', '$0x6', '$1', '$1$', '$10', '$100', '$100+', '$1000', '$100K', '$100M', '$100NL', '$100k', '$100m', '$100s', '$101', '$102', '$103', '$104', '$104$', '$105', '$105$', '$106', '$107', '$108', '$108MM', '$109', '$10Highlight', '$10This', '$10m', '$10million', '$11', '$110', '$110M', '$110million', '$111', '$112', '$113', '$114', '$115', '$116', '$117', '$118', '$119', '$11m', '$11million', '$12', '$120', '$1200', '$121', '$122', '$123', '$124', '$1240', '$125', '$126', '$127M', '$127m', '$128', '$129', '$12It', '$12M', '$12million', '$12tn', '$13', '$130', '$1300', '$130m', '$132', '$133', '$134', '$135', '$135M', '$137', '$138', '$139', '$13bn', '$13m', '$14', '$140', '$1400', '$140B', '$140bn', '$141', '$142', '$142M', '$143', '$144', '$145', '$146', '$147', '$147bn', '$148', '$149', '$1499', '$14bn', '$15', '$150', '$150M', '$150k', '$151', '$153', '$153M', '$154', '$155', '$155bn', '$156', '$157', '$158', '$159', '$15M', '$15MM', '$15k', '$15m', '$15mill', '$15million', '$16', '$160', '$160M', '$162', '$162bn', '$163', '$165', '$166bn', '$167', '$168', '$169', '$1690', '$16MM', '$16m', '$17', '$170', '$171', '$172', '$175', '$176', '$177', '$177m', '$178', '$179', '$17million', '$18', '$180', '$1800', '$181', '$181m', '$182', '$1830', '$184', '$185', '$186', '$187', '$188', '$189', '$18M', '$19', '$190', '$190k', '$191', '$192', '$193', '$195', '$196', '$198', '$199', '$19M', '$19m', '$1B+', '$1M', '$1bn', '$1k', '$1m', '$1mm', '$2', '$20', '$200', '$2000', '$200B', '$200M', '$200NL', '$200NL6', '$200m', '$201', '$202', '$205', '$206', '$207', '$208', '$20B', '$20K', '$20bn', '$20k', '$20m', '$20million', '$21', '$210', '$212', '$214', '$215', '$216', '$219', '$22', '$220', '$2200', '$222', '$224', '$225', '$227', '$23', '$230', '$233', '$234', '$235', '$236', '$24', '$240', '$241', '$243', '$244', '$245', '$246', '$249', '$25', '$25+', '$250', '$2500', '$250k', '$251', '$252', '$253', '$254', '$255', '$256', '$257', '$258', '$259', '$26', '$260', '$262', '$264', '$265', '$267', '$268', '$2699', '$27', '$270', '$2700', '$272', '$275', '$276', '$277', '$279', '$27bn', '$27m', '$28', '$280', '$282', '$285', '$288', '$289', '$29', '$290m', '$291', '$292', '$293', '$295', '$298', '$299', '$2MM', '$2^3$', '$2^6$', '$2bn', '$2j$', '$2k$', '$2m', '$2million', '$3', '$30', '$300', '$3000', '$300bn', '$300m', '$302', '$3025', '$306', '$307', '$30M', '$30k', '$31', '$310', '$311', '$312', '$313', '$315', '$319', '$32', '$32$', '$320', '$322', '$324', '$325', '$3260', '$3275', '$328', '$329', '$3290', '$33', '$330', '$332', '$334', '$3345', '$335', '$336', '$338', '$33M', '$34', '$340', '$341', '$342', '$3430', '$344', '$345', '$349', '$35', '$350', '$3500', '$350M', '$350k', '$353', '$354', '$355', '$359', '$35k', '$35m', '$36', '$360', '$360k', '$365', '$366', '$367', '$368', '$36MM', '$37', '$370', '$373', '$375', '$379', '$37M', '$37m', '$38', '$380', '$383', '$384', '$387', '$388', '$389', '$38MM', '$39', '$390', '$390m', '$392', '$393', '$394', '$395billion', '$396', '$399', '$3MM', '$3This', '$3b', '$3bn', '$3k', '$3m', '$4', '$40', '$400', '$400m', '$400mill', '$403', '$404', '$40k', '$40m', '$40million', '$41', '$410', '$412', '$413', '$415', '$419', '$41m', '$42', '$420', '$421', '$424', '$425', '$42M', '$42MM', '$42m', '$43', '$430', '$4300', '$432', '$434', '$435', '$436', '$439', '$44', '$440', '$442', '$445', '$446', '$448', '$44bn', '$45', '$450', '$4500', '$451', '$454', '$455', '$456', '$457', '$458', '$46', '$460', '$462', '$4655', '$466', '$47', '$470', '$472', '$474', '$475', '$476', '$478', '$478million', '$48', '$480', '$488', '$49', '$499', '$49k', '$49m', '$4M', '$4MM', '$4m', '$4million', '$5', '$5+', '$50', '$50+', '$500', '$500+', '$5000', '$500K', '$504', '$505', '$506', '$50m', '$50s', '$51', '$510', '$512', '$513', '$518', '$51m', '$52', '$524', '$525', '$525K', '$526', '$5275', '$528', '$53', '$530', '$533', '$535', '$539', '$54', '$540', '$542', '$543', '$545', '$547', '$548', '$549', '$54MM', '$54bn', '$55', '$550', '$550K', '$556', '$557', '$559', '$55M', '$56', '$560', '$561', '$5690', '$57', '$570', '$573', '$575', '$578', '$58', '$582', '$5825', '$584', '$586', '$589', '$59', '$590', '$590k', '$591', '$592', '$599', '$5M', '$5m', '$5million', '$6', '$60', '$600', '$6000', '$600bn', '$600m', '$606', '$608', '$60m', '$61', '$612', '$613', '$614', '$62', '$620', '$621', '$621K', '$622', '$624', '$627', '$62M', '$62bn', '$63', '$630', '$630m', '$635', '$64', '$641', '$643', '$645', '$646', '$649', '$64k', '$65', '$650', '$6500', '$650M', '$655', '$656', '$657', '$658', '$66', '$660', '$661', '$663', '$665', '$67', '$670', '$672', '$673', '$674', '$675', '$676', '$68', '$681', '$682', '$69', '$690', '$691', '$699', '$6M', '$6bn', '$6m', '$7', '$70', '$700', '$7000', '$704', '$70K', '$70MM', '$70k', '$70m', '$71', '$711', '$714', '$715', '$716', '$71M', '$72', '$720', '$723', '$72M', '$73', '$730', '$732', '$733', '$735', '$73m', '$74', '$749', '$75', '$750', '$7500', '$751', '$753', '$758', '$76', '$760', '$765', '$766', '$767', '$768', '$77', '$770', '$772', '$773', '$775', '$778', '$78', '$780', '$780m', '$781', '$782', '$787', '$788', '$789', '$79', '$795', '$799', '$7M', '$7m', '$7million', '$8', '$8$', '$80', '$800', '$8000', '$801', '$803', '$804', '$805', '$81', '$819', '$82', '$821', '$822', '$823', '$825', '$826', '$83', '$84', '$845', '$846', '$84M', '$85', '$850', '$853', '$855', '$858', '$86', '$860', '$862', '$865', '$86MM', '$87', '$875', '$878', '$88', '$880', '$881', '$888', '$889', '$89', '$890', '$898', '$8m', '$8million', '$9', '$90', '$900', '$9000', '$900M', '$905', '$90MM', '$91', '$910', '$913', '$914', '$91M', '$92', '$923', '$925', '$926', '$927', '$928', '$93', '$930', '$931', '$937', '$938', '$94', '$940', '$949', '$949m', '$95', '$950', '$950M', '$957', '$95m', '$96', '$96$', '$969', '$97', '$976', '$98', '$988', '$99', '$996', '$999', '$9M', '$9MM', '$9m', '$ADV', '$AVAGE', '$Action', '$AllPS', '$BBDPath', '$BIT', '$BLUE', '$BabadookMutex', '$BabadookMutexName', '$CharSet', '$CommandArguments', '$CurrMachineCmdPath', '$D$', '$ForegroundWindow', '$GF', '$GREEN', '$Global', '$H$', '$HITLOAD$', '$HOME', '$Hoophall', '$Interval', '$LIST', '$LogPath', '$MAIL', '$MutexWasCreated', '$MyPID', '$NOFORMAT', '$NewName', '$NewPath', '$O$', '$P$', '$PATH', '$PLC', '$PS1', '$Proc', '$RED', '$RET', '$RVAL', '$ReturnLen', '$ScriptPath', '$SharePath', '$StringBuffer', '$T$', '$TMPDIR', '$True', '$U', '$US', '$US1', '$US100', '$US14', '$US20', '$US200', '$US360', '$US500', '$US89', '$US900', '$Watchdog', '$WindowText', '$WindowTextLen', '$a', '$a+$b', '$accessor', '$adapter', '$allowedFields', '$atan', '$autoloader', '$b', '$broadcast', '$c', '$cells', '$ch', '$character', '$collection', '$compile', '$conditions', '$customer', '$customerQuery', '$d', '$data', '$dataMapper', '$destroy', '$doit', '$el', '$email', '$entities', '$entity', '$entityTable', '$env', '$extra', '$false', '$field', '$fields', '$file', '$fileName', '$hill', '$id', '$ign', '$input', '$j$', '$j+k$', '$k', '$k$', '$k+k=2k$', '$key', '$log', '$message', '$mutator', '$mymachine', '$n', '$n=106$', '$nakes', '$name', '$new', '$null', '$object', '$objfile', '$old', '$onChanges', '$onDestroy', '$onInit', '$output', '$p', '$p$', '$post', '$postLink', '$postlink', '$randStr', '$regInfo', '$result', '$result=curl', '$ret', '$role', '$rootFolder', '$row', '$rows', '$s', '$scope', '$script', '$service', '$settings', '$state', '$storage', '$sum', '$table', '$target', '$taskDefinition', '$taskRunAsUser', '$taskRunAsuser', '$tates', '$tempStorage', '$this', '$triggerDaily', '$triggerIdle', '$triggerLogon', '$triggers', '$true', '$type', '$unitOfWork', '$upload', '$user1', '$user2', '$user3', '$user4', '$v', '$value', '$vm', '$vm=get', '$vms=get', '$vms|', '$watch', '$x', '$x$', '$xHD$', '$x^2', '$y', '$z', '$z8', '%', '&', \"'\", '(', ')', '*', '+', '+$', '+$0', '+$1', '+$100', '++', '+++', '++++', '++=', '+0', '+0000', '+0100', '+0200', '+0x110', '+0x15a', '+0x197', '+0x1ea', '+0x227', '+0x25d', '+0x26e', '+0x283', '+0x2b8', '+0x38', '+0x489', '+0x546', '+0x5a', '+0x61', '+0x66', '+0x79', '+0x7b5', '+0x80', '+0x95', '+0xc8', '+0xd0', '+1', '+10', '+1000', '+101', '+11', '+12', '+127', '+128', '+13', '+14', '+141', '+145', '+146', '+15', '+150', '+156', '+16', '+166', '+17', '+18', '+19', '+192', '+2', '+20', '+200', '+20°C', '+21', '+22', '+24', '+25', '+250', '+27', '+29', '+3', '+30', '+300', '+31°46', '+31°47', '+32', '+35']\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_tokens))\n",
    "print(unique_tokens[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9a1d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'+2' in unique_tokens  # checking for token existence in the list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb59ab2",
   "metadata": {},
   "source": [
    "- The tokens are not what's desired, number-symbols-strings are not broken properly, so need to update the regular expression. \n",
    "\n",
    "(I'm sure there are advance ways to build better tokenizers, but here I'm trying to build as robust tokenizer as possible using the basic tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a413cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a225ddd4ae014fb58edf61c464d74787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'tokens'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#updating the tokenizer function further, to improve robustness \n",
    "#update-3\n",
    "\n",
    "def text_to_token(text):\n",
    "    tokens = regex.findall(r'\\p{L}+|\\p{N}+|[\\p{S}\\p{P}]', text[\"text\"]) #regex.findall(...) to match only the desired tokens directly, giving cleaner token boundaries.\n",
    "    return {\"tokens\": tokens}\n",
    "\n",
    "tokenized_ds = train_ds.map(text_to_token)\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d75fa",
   "metadata": {},
   "source": [
    "Explanation of pattern in the above regular expression:\n",
    "\n",
    "\\p{L}+ → one or more Unicode letters (e.g., words)\n",
    "\n",
    "\\p{N}+ → one or more Unicode numbers\n",
    "\n",
    "[\\p{S}\\p{P}] = all symbols and punctuation (each as a single character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f2d34530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '00', '000', '0000', '00000', '000000', '00000000', '000000000000', '0000000000000', '00000000000000', '0000000000000000', '0000000000000030', '00000000001', '00000001', '00000003', '00000008', '00000009', '0000001', '000000125', '00000016', '0000004', '000001', '000002', '000004', '000005', '000006', '00001', '0001', '000142', '00017', '0002', '00021', '000213', '00025', '0003', '000387', '0004', '0005', '0006', '00070', '00075', '00076125', '00081', '001', '0010', '0011', '001132', '0012', '00120', '00125', '0019', '00190', '002', '00221', '0023', '00238', '0026', '0026358', '00276319', '0029', '003', '0033', '0037', '004', '004123', '00416', '0042', '0044', '0048', '005', '005901', '005930', '006', '00658', '0067', '00684', '0069', '007', '0070', '00701532', '0073', '0075', '0079', '008', '00814037', '0087', '00872', '0089', '009', '00957359', '00996', '01', '010', '0100', '0101', '0101000020', '01014', '0103', '01034423', '0107', '0107706', '01085', '010964', '011', '0110627', '0112', '0115', '012', '0123456789', '012389', '01245', '012472', '012521', '01258', '0128007617', '013', '01363', '014', '0143', '01441', '014589', '0148', '01491', '015', '016', '016008', '016504', '017', '018', '01800565', '01806682', '01812464', '01839', '0189', '019', '01902442', '01902443', '0192', '01931769', '0199744505', '02', '020', '0200', '02000000', '020000005', '0202', '0203', '0204', '0207', '021', '0211', '02142', '0217', '0219', '022', '0224', '0225', '0226726347', '023', '0230', '023663', '024', '0242', '024218', '02426', '02438', '0246', '025', '026', '026201856', '0267', '027', '0272', '028', '029', '029401090870231288509578454981440888629750522601069337564316', '0295', '03', '030', '0300', '0300176481', '0302617', '030317', '03059393', '031', '03100263', '03106477', '0313', '0314', '031980', '032', '03283', '033', '0330', '0333', '034', '034243', '0348', '034932', '035', '036', '036675', '037', '038', '0381', '0385509480', '039', '04', '040', '0400', '0402', '0407', '041', '042', '0427', '043', '0430', '044', '045', '046', '046290', '046852', '047', '048', '048095', '049', '05', '050', '0500', '0501', '051', '051406', '052', '0520', '053']\n"
     ]
    }
   ],
   "source": [
    "all_tokens = [token for row in tokenized_ds \n",
    "                        for token in row[\"tokens\"]]\n",
    "\n",
    "unique_tokens = sorted(set(all_tokens))\n",
    "\n",
    "print(unique_tokens[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9751a26",
   "metadata": {},
   "source": [
    "'0000000000000030' etc. might come from:\n",
    "- Code dumps\n",
    "\n",
    "- Malformed content\n",
    "\n",
    "- Padding or spammy text\n",
    "\n",
    "- Machine-generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a7b3cc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175292\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03b96dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens found in vocab: []\n"
     ]
    }
   ],
   "source": [
    "# checking for previously generated unwanted token existence in the current unique_tokens list\n",
    "unwanted_tokens = ['<','$$', '$$$$', '$$3222111000', '$0', '$0$', '$0`', '$2MM', '$2^3$', '$2^6$', '$0x0', '$0x1', '$0x1020', '$0x20', '+2', '$0x5', '$0x6', '$1', '$105', '$105$', '$106', '$107', '$108', '$108MM', '$109', '$10Highlight', '$10This', '$10m', '$10million', '$19', '$190', '$190k']\n",
    "found = [token for token in unwanted_tokens if token in unique_tokens]\n",
    "print(\"Tokens found in vocab:\", found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61ad82d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ｍＷ程度と小さいので', 'ｐＦでＩＢＳに同調しました', '～', '｡', '･', 'ｲｨ', 'ﾟ', '￼', '�', '🇨', '🇳', '🇸', '🇺', '🌐', '🌹', '🎄', '🎉', '🏁', '🏻', '🏼', '🏽', '🏿', '🐒', '👀', '👋', '👏', '👑', '💀', '💃', '📊', '🔍', '🔥', '🖕', '🖖', '🗽', '😀', '😉', '😎', '😩', '😮', '😱', '😺', '🙁', '🙂', '🙌', '🤓', '🤔', '🤷', '🥖', '🦆']\n"
     ]
    }
   ],
   "source": [
    "print(unique_tokens[-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847dd3a6",
   "metadata": {},
   "source": [
    "Everything looks good, so proceeding futher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bb270efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tokens.extend(['<|unk|>', '<|endoftext|>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4ef8f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new vocabulary with more tokens\n",
    "new_vocab = {token: idx for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a6b2f68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175294\n"
     ]
    }
   ],
   "source": [
    "print(len(new_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f415568",
   "metadata": {},
   "source": [
    "### Creating a Tokenizer class, which will convert the input to Tokens and TokenIDs, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e5ee3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer_1:\n",
    "    def __init__(self, new_vocab, unk_token=\"<|unk|>\"): \n",
    "        self.str_to_int = new_vocab\n",
    "        self.int_to_str = {i:s for s,i in new_vocab.items()} #extracting words associated with tokenID produced by LLM \n",
    "        self.unk_token = unk_token\n",
    "\n",
    "    def encode(self, text): \n",
    "        preprocessed = regex.findall(r'\\p{L}+|\\p{N}+|[\\p{S}\\p{P}]', text) #creating tokens\n",
    "        # preprocessed = [item.strip() for item in preprocessed if item.strip()] # removing spaces\n",
    "        \n",
    "        #handeling tokens not present in vocab\n",
    "        # preprocessed = [\n",
    "        #     item if item in self.str_to_int #item represent a token in our list preprocessed, if that token exist in vocab it stays as it is\n",
    "        #     else self.unk_token for item in preprocessed # else it is converted to <|unk|>\n",
    "        # ]\n",
    "\n",
    "        #this update in ids helps avoid the loop just above\n",
    "        ids = [self.str_to_int.get(t, self.str_to_int[self.unk_token]) for t in preprocessed] #as we already have a dictionary of words, we'll extract id from vocab for the input we get. \n",
    "        return ids\n",
    "        \n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # text = regex.findall(r'\\p{L}+|\\p{N}+|[\\p{S}\\p{P}]', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4214a0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40637, 135488, 0, 41745, 99696, 171544, 115331, 4481]\n"
     ]
    }
   ],
   "source": [
    "#testing tokenizer\n",
    "adv_tokenizer = Tokenizer_1(new_vocab)\n",
    "\n",
    "print(adv_tokenizer.encode(\"Hey man! How are you doing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6369eb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey man ! How are you doing ?\n"
     ]
    }
   ],
   "source": [
    "print(adv_tokenizer.decode(adv_tokenizer.encode(\"Hey man! How are you doing?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4f89a8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40279, 175294, 11, 175294, 127275, 175294, 67348]\n"
     ]
    }
   ],
   "source": [
    "print(adv_tokenizer.encode(\"Hello baccho, aaj hum padhenge Physics\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello <|unk|> , <|unk|> hum <|unk|> Physics\n"
     ]
    }
   ],
   "source": [
    "#testing for some possible unknown words\n",
    "print(adv_tokenizer.decode(adv_tokenizer.encode(\"Hello baccho, aaj hum padhenge Physics\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657334bc",
   "metadata": {},
   "source": [
    "- This highlights that despite the vocab consisting of 175292, there are many possible words that aren't present in our vocab. \n",
    "- This simply explains the scale of data and tokens that are require to train a realworld LLM.\n",
    "- Also, as the above output is displayed, it's not the best way to handel unknown tokens. \n",
    "- To overcome the above limitations with tokens and unknown token, Byte-pair encoding will help us make the system more robust. \n",
    "\n",
    "\n",
    "(on that next) :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775b6406",
   "metadata": {},
   "source": [
    "### Some Important points:\n",
    "\n",
    "#### 1. Is removing whitespace a good practice in tokenization?\n",
    "It depends on your tokenizer’s philosophy.\n",
    "- In simple tokenizers like one implemented here, its okay to do so.\n",
    "\n",
    "🔹 In modern LLM tokenizers (like GPT, BERT):\n",
    "Whitespace is kept as meaningful context using special handling:\n",
    "\n",
    "GPT's tokenizer (like Byte Pair Encoding or Tiktoken) often treats \" hello\" and \"hello\" as different tokens.\n",
    "\n",
    "It may encode the leading space into the token (e.g., \" hello\" becomes one token that includes the space).\n",
    "\n",
    "Spaces can influence token boundaries and are not always discarded.\n",
    "\n",
    "\n",
    "#### 2. Are capitalized and lowercase words treated the same? (“Hello” vs “hello”)\n",
    "No, they are treated as different tokens unless you normalize them.\n",
    "\n",
    "🔹 Examples:\n",
    "\"Hello\" and \"hello\" will be tokenized differently in GPT, BERT, and most tokenizers.\n",
    "\n",
    "If you want case-insensitive tokens (e.g., for small LLMs or your own experiments), you can preprocess by lowercasing: text = text.lower()\n",
    "\n",
    "But most modern LLMs are case-sensitive, because:\n",
    "\n",
    "- Case carries semantic weight (e.g., \"Apple\" the company vs \"apple\" the fruit)\n",
    "\n",
    "- Preserving case helps capture nuances in proper nouns, titles, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c4acc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-env)",
   "language": "python",
   "name": "llm-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
